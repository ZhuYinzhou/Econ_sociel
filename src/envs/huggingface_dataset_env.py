import gym
from gym import spaces
import numpy as np
import os
from datasets import load_dataset, Dataset, IterableDataset, DatasetDict
try:
    from datasets import load_from_disk  # type: ignore
except Exception:
    load_from_disk = None  # type: ignore
from typing import Dict, Any, Optional, Tuple, List
from loguru import logger
import re
import torch
import torch.nn.functional as F
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import random

class HuggingFaceDatasetEnv(gym.Env):
    metadata = {'render_modes': ['human'], 'render_fps': 4}

    def __init__(self, **kwargs):
        super().__init__()
        
        self.dataset_path = kwargs.get("hf_dataset_path", "gsm8k")
        self.dataset_config_name = kwargs.get("hf_dataset_config_name", None)
        self.dataset_split = kwargs.get("dataset_split", "train")
        self.is_streaming = kwargs.get("dataset_streaming", False)
        self.use_random_sampling = kwargs.get("use_random_sampling", True)  # æ·»åŠ éšæœºé‡‡æ ·é€‰é¡¹
        self.use_dataset_episode = kwargs.get("use_dataset_episode", False)  # æ•°æ®é›†çº§åˆ«episodeé€‰é¡¹
        # Optional discrete-action size for training (e.g., stance classification K=3)
        # Note: action_space remains Text; this n_actions is for runner/buffer/action_selector shapes.
        self.n_actions = int(kwargs.get("n_actions", 1))

        # Optional filter for datasets that contain both core/non-core samples
        # - None: use all samples
        # - True: only samples with sample["is_core_user"] == True
        # - False: only samples with sample["is_core_user"] == False
        self.filter_is_core_user = kwargs.get("filter_is_core_user", None)
        if isinstance(self.filter_is_core_user, str):
            s = self.filter_is_core_user.strip().lower()
            if s in ("core", "true", "1", "yes"):
                self.filter_is_core_user = True
            elif s in ("noncore", "non-core", "false", "0", "no"):
                self.filter_is_core_user = False
            else:
                self.filter_is_core_user = None
        
        self.question_field = kwargs.get("question_field_name", "question")
        self.answer_field = kwargs.get("answer_field_name", "answer")
        
        # For reward calculation, if needed directly in env
        self.reward_args = kwargs.get("reward_config", {})
        # Whether to emit belief/z supervision fields from samples (for z_transition training)
        self.emit_belief_fields = bool(kwargs.get("emit_belief_fields", True))
        # Optional: population belief dim for offline transition supervision.
        # - K=3: categorical distribution over stances
        # - K=1: scalar z in [-1,1]
        self.population_belief_dim = int(kwargs.get("population_belief_dim", 3))
        self.population_belief_dim = max(1, self.population_belief_dim)

        try:
            # Support both:
            # - HuggingFace hub datasets via load_dataset(...)
            # - local datasets saved by DatasetDict.save_to_disk(...) via load_from_disk(...)
            use_local_disk = bool(isinstance(self.dataset_path, str) and os.path.isdir(self.dataset_path))
            if use_local_disk:
                if self.is_streaming:
                    logger.warning("dataset_streaming=True is not supported for load_from_disk datasets. Forcing streaming=False.")
                    self.is_streaming = False
                if load_from_disk is None:
                    raise RuntimeError("datasets.load_from_disk is unavailable. Please ensure `datasets` is installed correctly.")
                loaded = load_from_disk(self.dataset_path)
                # loaded can be Dataset or DatasetDict
                if isinstance(loaded, DatasetDict):
                    if self.dataset_split not in loaded:
                        raise KeyError(f"Split '{self.dataset_split}' not found in dataset at {self.dataset_path}. Available: {list(loaded.keys())}")
                    self.dataset = loaded[self.dataset_split]
                else:
                    # If it's a single Dataset, treat it as the requested split
                    self.dataset = loaded
                logger.info(f"Loaded dataset from disk: {self.dataset_path}, split: {self.dataset_split}, streaming={self.is_streaming}")
            else:
                self.dataset = load_dataset(
                    self.dataset_path,
                    name=self.dataset_config_name,
                    split=self.dataset_split,
                    streaming=self.is_streaming
                )
            if self.is_streaming:
                self.dataset_iterator = iter(self.dataset)
                # For IterableDataset, we can't easily get the length.
                # We might need a max_episodes arg from config for termination in streaming mode.
                logger.info(f"Loaded IterableDataset: {self.dataset_path}, split: {self.dataset_split}")
            else:
                self.dataset_list = list(self.dataset) # Convert to list for easier iteration and shuffling if needed
                # Apply optional filter on samples
                if self.filter_is_core_user is not None:
                    want = bool(self.filter_is_core_user)
                    before = len(self.dataset_list)
                    self.dataset_list = [s for s in self.dataset_list if bool(s.get("is_core_user", False)) == want]
                    after = len(self.dataset_list)
                    logger.info(f"Filtered dataset by is_core_user={want}: {before} -> {after}")
                self.dataset_iterator = None # Will be created in reset
                self.current_data_idx = -1
                self.num_samples = len(self.dataset_list)
                logger.info(f"Loaded Dataset: {self.dataset_path}, split: {self.dataset_split}, num_samples: {self.num_samples}")
                
                # å¦‚æœä½¿ç”¨éšæœºé‡‡æ ·ï¼Œæ‰“ä¹±æ•°æ®é›†
                if self.use_random_sampling and not self.use_dataset_episode:
                    random.shuffle(self.dataset_list)
                    logger.info("Dataset shuffled for random sampling")

        except Exception as e:
            logger.error(f"Failed to load dataset '{self.dataset_path}' (config: {self.dataset_config_name}, split: {self.dataset_split}): {e}")
            raise

        self.max_question_length = kwargs.get("max_question_length", 1024)
        self.max_answer_length = kwargs.get("max_answer_length", 1024) # For action space

        # Define action space - what the agent "outputs" to the environment
        # For LLMs, this is typically the generated text.
        # Using gym.spaces.Text requires gym version 0.26+
        # As a placeholder, or if direct text passing is used, can be simplified.
        self.action_space = spaces.Text(max_length=self.max_answer_length)

        # Define observation space - what the agent "sees"
        # This will be the question text.
        self.observation_space = spaces.Text(max_length=self.max_question_length)
        
        # Current sample from the dataset
        self.current_sample: Optional[Dict] = None
        self.current_question: Optional[str] = None
        self.current_ground_truth_answer: Optional[str] = None
        self.episode_count = 0  # æ·»åŠ episodeè®¡æ•°å™¨ï¼Œç”¨äºè¿½è¸ªé—®é¢˜
        
        # æ•°æ®é›†çº§åˆ«episodeçš„çŠ¶æ€è¿½è¸ª
        if self.use_dataset_episode:
            self.step_count = 0  # å½“å‰episodeå†…çš„æ­¥æ•°
            self.episode_limit = self.num_samples if not self.is_streaming else 1000  # ä½¿ç”¨æ•°æ®é›†å¤§å°ä½œä¸ºepisodeé™åˆ¶
            self.current_episode_samples = []  # å½“å‰episodeå¤„ç†çš„æ‰€æœ‰æ ·æœ¬
            self.episode_results = []  # å½“å‰episodeçš„æ‰€æœ‰ç»“æœ
        else:
            # Episode specifics (each question is an episode)
            self.episode_length = 0 # Steps within current episode (always 1)
            self.episode_limit = 1

    def _get_next_sample(self) -> Optional[Dict]:
        if self.is_streaming:
            try:
                # In streaming mode, loop until we find a matching sample if filtering is enabled
                while True:
                    sample = next(self.dataset_iterator)
                    if self.filter_is_core_user is None:
                        return sample
                    want = bool(self.filter_is_core_user)
                    if bool(sample.get("is_core_user", False)) == want:
                        return sample
            except StopIteration:
                logger.info("Streaming dataset iterator exhausted.")
                return None
        else:
            if self.use_dataset_episode:
                # æ•°æ®é›†çº§åˆ«episodeï¼šé¡ºåºéå†æ‰€æœ‰æ ·æœ¬
                self.current_data_idx += 1
                if self.current_data_idx < self.num_samples:
                    return self.dataset_list[self.current_data_idx]
                else:
                    logger.info("Dataset-level episode completed: all samples processed.")
                    return None
            elif self.use_random_sampling:
                # éšæœºé‡‡æ ·ï¼šæ¯æ¬¡éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
                if self.num_samples > 0:
                    random_idx = random.randint(0, self.num_samples - 1)
                    sample = self.dataset_list[random_idx]
                    logger.debug(f"Random sampling: selected index {random_idx}")
                    return sample
                else:
                    logger.info("Dataset is empty.")
                    return None
            else:
                # é¡ºåºé‡‡æ ·ï¼šåŸæœ‰é€»è¾‘
                self.current_data_idx += 1
                if self.current_data_idx < self.num_samples:
                    return self.dataset_list[self.current_data_idx]
                else:
                    logger.info("Non-streaming dataset iterator exhausted.")
                    return None

    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[Any, Dict[str, Any]]:
        super().reset(seed=seed) # Gym 0.26+
        
        # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
        
        if self.use_dataset_episode:
            # æ•°æ®é›†çº§åˆ«episodeï¼šé‡ç½®çŠ¶æ€ï¼Œä»ç¬¬ä¸€ä¸ªæ ·æœ¬å¼€å§‹
            self.current_data_idx = -1  # ä¼šè¢«_get_next_sampleé€’å¢åˆ°0
            self.step_count = 0
            self.current_episode_samples = []
            self.episode_results = []
            self.episode_count += 1
            logger.info(f"Starting dataset-level Episode {self.episode_count}: will process {self.num_samples} samples")
        else:
            # åŸæœ‰é€»è¾‘ï¼šæ¯ä¸ªé—®é¢˜ä¸€ä¸ªepisode
            if not self.is_streaming and (self.dataset_iterator is None or self.current_data_idx >= self.num_samples -1):
                # Re-initialize iterator for non-streaming dataset if exhausted or first time
                if not self.use_random_sampling:
                    # åªæœ‰åœ¨é¡ºåºé‡‡æ ·æ—¶æ‰é‡ç½®ç´¢å¼•
                    self.current_data_idx = -1 # Will be incremented by _get_next_sample

        self.current_sample = self._get_next_sample()
        
        if self.current_sample is None:
            # Handle dataset exhaustion, e.g., by raising an error or returning a special state
            # For now, let's raise an error to make it explicit during development.
            # In a long run, might want to loop the dataset or have a max_episodes from config.
            raise StopIteration("Dataset exhausted. Implement looping or max_episode limit if needed.")

        self.current_question = str(self.current_sample.get(self.question_field, ""))
        self.current_ground_truth_answer = str(self.current_sample.get(self.answer_field, ""))
        
        if not self.use_dataset_episode:
            self.episode_length = 0
            self.episode_count += 1
        
        # è®°å½•é—®é¢˜å˜åŒ–ä»¥ä¾¿è°ƒè¯•
        if self.use_dataset_episode:
            question_preview = self.current_question[:100] + "..." if len(self.current_question) > 100 else self.current_question
            logger.info(f"Episode {self.episode_count}, Step {self.step_count + 1}/{self.num_samples}: {question_preview}")
        else:
            question_preview = self.current_question[:100] + "..." if len(self.current_question) > 100 else self.current_question
            logger.info(f"Episode {self.episode_count}: New question - {question_preview}")
        
        # Observation is the question text
        # Preprocess if necessary (e.g., tokenization if obs_space was Box)
        # For now, passing raw text, MAC needs to handle it.
        observation = self.current_question 
        
        info = {"sample": self.current_sample} # Pass the whole sample for potential use in reward or logging
        return observation, info

    def get_belief_tensor(self, belief_inputs: Optional[Dict[str, Any]], device: Optional[torch.device] = None) -> Dict[str, torch.Tensor]:
        """
        A minimal tensorizer to match EpisodeRunner._get_post_transition_data() expectations.
        This enables offline z_transition supervision by providing population_z (K=3 categorical or K=1 scalar).
        """
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        bi = belief_inputs if isinstance(belief_inputs, dict) else {}
        pop_dim = int(getattr(self, "population_belief_dim", 3))
        pop_dim = max(1, pop_dim)
        pz_raw = bi.get("population_z")
        if pz_raw is None:
            pz_raw = bi.get("z_t")

        # population_z
        if pop_dim == 1:
            # scalar in [-1,1]
            try:
                if isinstance(pz_raw, torch.Tensor):
                    z = float(pz_raw.detach().flatten()[0].item())
                elif isinstance(pz_raw, (list, tuple)) and len(pz_raw) > 0:
                    z = float(pz_raw[0])
                else:
                    z = float(pz_raw) if pz_raw is not None else 0.0
            except Exception:
                z = 0.0
            z = float(max(-1.0, min(1.0, z)))
            pz = [z]
        else:
            # categorical simplex (default K=3)
            pz = pz_raw if pz_raw is not None else [1.0 / float(pop_dim) for _ in range(pop_dim)]
            try:
                pz = [float(x) for x in list(pz)[:pop_dim]]
            except Exception:
                pz = [1.0 / float(pop_dim) for _ in range(pop_dim)]
            s = float(sum(max(0.0, x) for x in pz))
            if s <= 0:
                pz = [1.0 / float(pop_dim) for _ in range(pop_dim)]
            else:
                pz = [max(0.0, x) / s for x in pz]

        # neighbor stance counts (optional; keep zeros)
        nb = bi.get("neighbor_stance_counts") or [0, 0, 0]
        try:
            nb = [int(x) for x in list(nb)[:3]]
        except Exception:
            nb = [0, 0, 0]

        is_core = bi.get("is_core_user", False)
        try:
            is_core = bool(is_core)
        except Exception:
            is_core = False

        return {
            "population_z": torch.tensor(pz, dtype=torch.float32, device=device),
            "neighbor_stance_counts": torch.tensor(nb, dtype=torch.float32, device=device),
            "is_core_user": torch.tensor([1 if is_core else 0], dtype=torch.int64, device=device),
        }

    def step(self, action: Any, extra_info: Optional[Dict[str, Any]] = None) -> Tuple[Any, float, bool, bool, Dict[str, Any]]:
        """
        Execute one step in the environment.
        
        Args:
            action: The primary action (final answer string from coordinator/agent)
            extra_info: Additional information for reward calculation including:
                - 'agent_responses': List[str] - Individual agent responses
                - 'commitment_text': str - Coordinator's commitment text
                - 'agent_log_probs': Optional[List[float]] - Token log probabilities for AL reward
                - 'prompt_embeddings': Optional[torch.Tensor] - Agent prompt embeddings
                - 'belief_states': Optional[torch.Tensor] - Agent belief states
        """
        if self.current_sample is None:
            raise RuntimeError("step() called before reset() or after dataset exhaustion.")

        if self.use_dataset_episode:
            self.step_count += 1
        else:
            self.episode_length += 1
        
        # Extract primary action and additional info
        if isinstance(action, dict):
            llm_answer_str = str(action.get("answer", ""))
            if extra_info is None:
                extra_info = action  # Use action dict as extra_info if not provided separately
        else:
            llm_answer_str = str(action)
            
        if extra_info is None:
            extra_info = {}

        # === æ¸…æ™°æ˜¾ç¤ºå®Œæ•´æ¨ç†è¿‡ç¨‹ ===
        logger.info("=" * 80)
        logger.info(f"ğŸ” QUESTION: {self.current_question}")
        logger.info("=" * 80)
        
        # Strategy will be logged by MAC
        
        # Executor responses will be logged by MAC
        
        # Coordinator commitment will be logged by MAC
        
        logger.info(f"ğŸ“– GROUND TRUTH: {self.current_ground_truth_answer}")
        logger.info("=" * 80)

        # --- Reward Calculation ---
        # æƒé‡ï¼ˆå¦‚æœæŸé¡¹æƒé‡ä¸º 0ï¼Œåˆ™è·³è¿‡è¯¥é¡¹çš„æ˜‚è´µè®¡ç®—ï¼Œå°¤å…¶æ˜¯ TF-IDF ç›¸ä¼¼åº¦ï¼‰
        al_weight = float(getattr(self.reward_args, "al_weight", 0.3))
        ts_weight = float(getattr(self.reward_args, "ts_weight", 0.5))
        cc_weight = float(getattr(self.reward_args, "cc_weight", 0.2))

        # Task-Specific (TS) reward: åŸºäºæ­£ç¡®æ€§ï¼ˆå¯¹ stance_id/boxed-id ç›‘ç£æœ€å¸¸ç”¨ï¼‰
        is_correct = self._evaluate_answer(llm_answer_str, self.current_ground_truth_answer)
        reward_ts = 1.0 if is_correct else 0.0

        # Action Likelihood (AL) reward: ä»…å½“æƒé‡>0 æ—¶è®¡ç®—
        reward_al = 0.0
        if al_weight > 0:
            reward_al = self._calculate_action_likelihood_reward(extra_info)

        # Collaborative Contribution (CC) reward: ä»…å½“æƒé‡>0 æ—¶è®¡ç®—
        reward_cc = 0.0
        if cc_weight > 0:
            reward_cc = self._calculate_collaborative_contribution_reward(
                llm_answer_str, extra_info, is_correct
            )

        total_reward = al_weight * float(reward_al) + ts_weight * float(reward_ts) + cc_weight * float(reward_cc)
        
        # æ˜¾ç¤ºå¥–åŠ±ä¿¡æ¯
        logger.info(f"ğŸ¯ REWARD BREAKDOWN:")
        logger.info(f"   TS (Task-Specific): {reward_ts:.3f} * {ts_weight:.1f} = {reward_ts * ts_weight:.3f}")
        logger.info(f"   AL (Action Likelihood): {reward_al:.3f} * {al_weight:.1f} = {reward_al * al_weight:.3f}")
        logger.info(f"   CC (Collaborative): {reward_cc:.3f} * {cc_weight:.1f} = {reward_cc * cc_weight:.3f}")
        logger.info(f"   TOTAL REWARD: {total_reward:.3f}")
        logger.info("=" * 80)
        
        # --- æ•°æ®é›†çº§åˆ«episodeçš„ç‰¹æ®Šå¤„ç† ---
        if self.use_dataset_episode:
            # è®°å½•å½“å‰æ ·æœ¬ç»“æœ
            step_result = {
                "question": self.current_question,
                "ground_truth": self.current_ground_truth_answer,
                "llm_answer": llm_answer_str,
                "is_correct": is_correct,
                "reward_ts": reward_ts,
                "reward_al": reward_al,
                "reward_cc": reward_cc,
                "total_reward": total_reward
            }
            self.current_episode_samples.append(self.current_sample)
            self.episode_results.append(step_result)
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆæ•´ä¸ªæ•°æ®é›†
            terminated = (self.step_count >= self.num_samples)
            
            if not terminated:
                # è·å–ä¸‹ä¸€ä¸ªæ ·æœ¬
                self.current_sample = self._get_next_sample()
                if self.current_sample is None:
                    terminated = True
                else:
                    self.current_question = str(self.current_sample.get(self.question_field, ""))
                    self.current_ground_truth_answer = str(self.current_sample.get(self.answer_field, ""))
            
            if terminated:
                # Episodeç»“æŸï¼Œè®¡ç®—æ•´ä½“ç»Ÿè®¡
                total_correct = sum(1 for r in self.episode_results if r["is_correct"])
                accuracy = total_correct / len(self.episode_results) if self.episode_results else 0.0
                avg_reward = sum(r["total_reward"] for r in self.episode_results) / len(self.episode_results) if self.episode_results else 0.0
                
                logger.info(f"ğŸ“Š DATASET-LEVEL EPISODE {self.episode_count} COMPLETED:")
                logger.info(f"   Total samples: {len(self.episode_results)}")
                logger.info(f"   Correct answers: {total_correct}")
                logger.info(f"   Accuracy: {accuracy:.3f}")
                logger.info(f"   Average reward: {avg_reward:.3f}")
                logger.info("=" * 80)
                
                next_observation = ""  # Episodeç»“æŸï¼Œæ— ä¸‹ä¸€ä¸ªè§‚å¯Ÿ
            else:
                next_observation = self.current_question  # ä¸‹ä¸€ä¸ªé—®é¢˜ä½œä¸ºä¸‹ä¸€ä¸ªè§‚å¯Ÿ
        else:
            # åŸæœ‰é€»è¾‘ï¼šæ¯ä¸ªé—®é¢˜ä¸€ä¸ªepisode
            terminated = True
            next_observation = ""  # Placeholder

        truncated = False # Not typically used if episode length is fixed at 1 or based on dataset size
        
        info = {
            "is_correct": is_correct,
            "reward_ts": reward_ts,
            "reward_al": reward_al,
            "reward_cc": reward_cc,
            "llm_answer": llm_answer_str,
            "ground_truth_answer": self.current_ground_truth_answer
        }

        # Optional: emit belief/z supervision fields for offline transition training
        if self.emit_belief_fields and isinstance(self.current_sample, dict):
            # stage index
            st = self.current_sample.get("stage_t", self.current_sample.get("t", 0))
            try:
                st = int(st)
            except Exception:
                st = 0
            info["t"] = int(st)

            pop_dim = int(getattr(self, "population_belief_dim", 3))
            pop_dim = max(1, pop_dim)

            # z supervision fields
            z_t = self.current_sample.get("z_t")
            z_target = self.current_sample.get("z_target")
            z_mask = self.current_sample.get("z_mask", 0.0)
            if pop_dim == 1:
                # scalar regression target
                try:
                    if isinstance(z_target, torch.Tensor):
                        zt = float(z_target.detach().flatten()[0].item())
                    elif isinstance(z_target, (list, tuple)) and len(z_target) > 0:
                        zt = float(z_target[0])
                    else:
                        zt = float(z_target) if z_target is not None else 0.0
                except Exception:
                    zt = 0.0
                info["z_target"] = [float(max(-1.0, min(1.0, zt)))]
            else:
                if isinstance(z_target, (list, tuple)) and len(z_target) >= pop_dim:
                    info["z_target"] = [float(x) for x in list(z_target)[:pop_dim]]
            if z_mask is not None:
                try:
                    info["z_mask"] = float(z_mask)
                except Exception:
                    info["z_mask"] = 0.0

            # Provide belief_inputs_{pre,post} so EpisodeRunner can call get_belief_tensor() and fill batch z_t / belief_pre_population_z
            if pop_dim == 1:
                # z_t / z_target can be scalar in dataset; keep as scalar and let get_belief_tensor() handle it.
                info["belief_inputs_pre"] = {
                    "population_z": z_t,
                    "is_core_user": bool(self.current_sample.get("is_core_user", False)),
                    "neighbor_stance_counts": [0, 0, 0],
                }
                info["belief_inputs_post"] = {
                    "population_z": z_target,
                    "is_core_user": bool(self.current_sample.get("is_core_user", False)),
                    "neighbor_stance_counts": [0, 0, 0],
                }
            else:
                if isinstance(z_t, (list, tuple)) and len(z_t) >= pop_dim:
                    info["belief_inputs_pre"] = {
                        "population_z": [float(x) for x in list(z_t)[:pop_dim]],
                        "is_core_user": bool(self.current_sample.get("is_core_user", False)),
                        "neighbor_stance_counts": [0, 0, 0],
                    }
                if isinstance(z_target, (list, tuple)) and len(z_target) >= pop_dim:
                    info["belief_inputs_post"] = {
                        "population_z": [float(x) for x in list(z_target)[:pop_dim]],
                        "is_core_user": bool(self.current_sample.get("is_core_user", False)),
                        "neighbor_stance_counts": [0, 0, 0],
                    }
        
        # ä¸ºæ•°æ®é›†çº§åˆ«episodeæ·»åŠ é¢å¤–ä¿¡æ¯
        if self.use_dataset_episode:
            info.update({
                "step_count": self.step_count,
                "total_steps": self.num_samples,
                "progress": self.step_count / self.num_samples
            })
            
            if terminated:
                # æ·»åŠ æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
                total_correct = sum(1 for r in self.episode_results if r["is_correct"])
                info.update({
                    "episode_accuracy": total_correct / len(self.episode_results) if self.episode_results else 0.0,
                    "episode_avg_reward": sum(r["total_reward"] for r in self.episode_results) / len(self.episode_results) if self.episode_results else 0.0,
                    "total_samples_processed": len(self.episode_results)
                })
        
        return next_observation, total_reward, terminated, truncated, info

    def _extract_boxed_content(self, text: str) -> Optional[str]:
        """Extracts content from \\boxed{} with improved fallback mechanisms."""
        if not isinstance(text, str): # Ensure text is a string
            return None
        
        # Primary: Look for \\boxed{content}
        match = re.search(r"\\boxed\{([\s\S]*?)\}", text)
        if match:
            content = match.group(1).strip()
            return content if content else None
        
        # Fallback 1: Look for boxed{content} without backslash
        match = re.search(r"boxed\{([\s\S]*?)\}", text)
        if match:
            content = match.group(1).strip()
            logger.info(f"Found 'boxed{{}}' without backslash: {content}")
            return content if content else None
        
        # Fallback 2: Look for the last number in the text (often the final answer)
        numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", text)
        last_number_candidate = numbers[-1] if numbers else None
        
        # Fallback 3: Look for "answer is" patterns, but prefer last number if found
        patterns = [
            r"(?:answer is|answer:|final answer is|final answer:|the answer is)\s*([+-]?\d+(?:\.\d+)?)",
            r"(?:therefore|thus|so)\s*[^0-9]*([+-]?\d+(?:\.\d+)?)",
            r"(?:equals|=)\s*([+-]?\d+(?:\.\d+)?)",
        ]
        
        pattern_matches = []
        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                pattern_matches.append(match.group(1))
        
        # Strategy: If we have both pattern matches and a last number, 
        # prefer the last number if it appears in the later part of the text
        if last_number_candidate and pattern_matches:
            # Check if the last number appears after the pattern matches
            last_number_pos = text.rfind(last_number_candidate)
            pattern_positions = []
            for pattern_match in pattern_matches:
                pos = text.rfind(pattern_match)
                if pos != -1:
                    pattern_positions.append(pos)
            
            # If last number appears after pattern matches, prefer it
            if pattern_positions and last_number_pos > max(pattern_positions):
                logger.info(f"Using last number as it appears after pattern matches: {last_number_candidate}")
                return last_number_candidate
            elif pattern_matches:
                logger.info(f"Using pattern match: {pattern_matches[0]}")
                return pattern_matches[0]
        
        # If only pattern matches exist
        if pattern_matches:
            logger.info(f"Using pattern match: {pattern_matches[0]}")
            return pattern_matches[0]
        
        # If only last number exists
        if last_number_candidate:
            logger.info(f"Using last number in text as fallback: {last_number_candidate}")
            return last_number_candidate
        
        logger.warning(f"No answer found in text: {text[:100]}...")
        return None

    def _normalize_number_string(self, s: Optional[str]) -> Optional[str]:
        """Normalizes a string potentially representing a number."""
        if s is None:
            return None
        # Remove commas used as thousand separators
        s_no_commas = s.replace(",", "")
        # Remove trailing ".0" or ".00" etc. to treat 123.0 as 123 for int comparison
        # but keep 123.5 as 123.5
        if '.' in s_no_commas:
            parts = s_no_commas.split('.')
            if len(parts) == 2 and all(c == '0' for c in parts[1]):
                return parts[0] # Return only integer part if fractional part is all zeros
        return s_no_commas

    def _evaluate_answer(self, llm_answer: str, ground_truth_answer: str) -> bool:
        logger.debug(f"Evaluating LLM Answer: '{llm_answer}' vs Ground Truth: '{ground_truth_answer}'")

        llm_boxed_content = self._extract_boxed_content(llm_answer)
        gt_boxed_content = self._extract_boxed_content(ground_truth_answer)

        logger.debug(f"Boxed Content - LLM: '{llm_boxed_content}', GT: '{gt_boxed_content}'")

        # å¦‚æœä¸¤ä¸ªéƒ½æ²¡æœ‰boxedå†…å®¹ï¼Œå°è¯•ç›´æ¥ä»æ–‡æœ¬ä¸­æå–æ•°å­—
        if llm_boxed_content is None and gt_boxed_content is None:
            logger.info("Both answers lack \\boxed{} format, attempting direct text comparison")
            # å°è¯•ä»æ–‡æœ¬ä¸­æå–æœ€åçš„æ•°å­—
            llm_numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", llm_answer)
            gt_numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", ground_truth_answer)
            
            if llm_numbers and gt_numbers:
                llm_boxed_content = llm_numbers[-1]
                gt_boxed_content = gt_numbers[-1]
                logger.info(f"Extracted numbers - LLM: '{llm_boxed_content}', GT: '{gt_boxed_content}'")
            else:
                logger.info(f"Evaluation failed: Unable to extract numerical answers. LLM: '{llm_answer[:100]}...', GT: '{ground_truth_answer[:100]}...'")
                return False

        # å¦‚æœåªæœ‰ä¸€ä¸ªæœ‰boxedå†…å®¹ï¼Œè¿™é€šå¸¸è¡¨ç¤ºæ ¼å¼é—®é¢˜
        if llm_boxed_content is None or gt_boxed_content is None:
            logger.info(f"Evaluation failed: Inconsistent answer formats. LLM boxed: '{llm_boxed_content}', GT boxed: '{gt_boxed_content}'")
            logger.info(f"Full answers - LLM: '{llm_answer[:150]}...', GT: '{ground_truth_answer[:150]}...'")
            return False

        # Normalize the string content from \boxed{} before attempting float conversion or string comparison
        norm_llm_content = self._normalize_number_string(llm_boxed_content)
        norm_gt_content = self._normalize_number_string(gt_boxed_content)
        
        logger.debug(f"Normalized Boxed Content - LLM: '{norm_llm_content}', GT: '{norm_gt_content}'")

        if norm_llm_content is None or norm_gt_content is None: # Should not happen if _extract_boxed_content returned non-None
             return False

        try:
            # Attempt to convert both to floats for numerical comparison
            llm_val = float(norm_llm_content)
            gt_val = float(norm_gt_content)

            # Check for near-equality
            if abs(llm_val - gt_val) < 1e-5:
                logger.info(f"âœ… Correct answer: {llm_val} == {gt_val}")
                return True
            else:
                logger.info(f"âŒ Numeric mismatch: LLM val {llm_val} vs GT val {gt_val}")
                return False
        except ValueError:
            # If conversion to float fails, fall back to string comparison
            logger.debug(f"ValueError converting to float. Comparing normalized strings: '{norm_llm_content}' vs '{norm_gt_content}'")
            if norm_llm_content == norm_gt_content:
                logger.info(f"âœ… Correct answer (string match): '{norm_llm_content}'")
                return True
            else:
                # Last resort: compare the original (just stripped) boxed content
                if llm_boxed_content.strip() == gt_boxed_content.strip():
                    logger.info(f"âœ… Correct answer (original content match): '{llm_boxed_content.strip()}'")
                    return True
                logger.info(f"âŒ String mismatch after float conversion failed. LLM: '{norm_llm_content}', GT: '{norm_gt_content}'")
                return False

    def _calculate_action_likelihood_reward(self, extra_info: Dict[str, Any]) -> float:
        """
        è®¡ç®—åŠ¨ä½œä¼¼ç„¶æ€§å¥–åŠ± r^AL
        åŸºäºä»¥ä¸‹å› ç´ ï¼š
        1. Agentå“åº”çš„ä¸€è‡´æ€§å’Œè´¨é‡
        2. å“åº”ä¸commitmentçš„ç›¸ä¼¼åº¦
        3. å“åº”çš„æ­£ç¡®æ€§ï¼ˆé€šè¿‡æ•°å­—åŒ¹é…æ£€æŸ¥ï¼‰
        4. å¦‚æœAPIå¤±è´¥æˆ–å“åº”æ— æ•ˆï¼Œè¿”å›0.0
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„agentå“åº”
            agent_responses = extra_info.get('agent_responses', [])
            commitment_text = extra_info.get('commitment_text', '')
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå“åº”æˆ–å“åº”åŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œè¿”å›0.0
            if not agent_responses or not commitment_text:
                return 0.0
            
            # æ£€æŸ¥å“åº”æ˜¯å¦åŒ…å«APIé”™è¯¯ä¿¡æ¯
            error_indicators = ['Error: Could not generate response', 'API Error', 'HTTP error', 'Failed to generate']
            for response in agent_responses:
                if any(error in str(response) for error in error_indicators):
                    return 0.0
            
            if any(error in str(commitment_text) for error in error_indicators):
                return 0.0
            
            # è¿‡æ»¤æ‰åŒ…å«é”™è¯¯çš„å“åº”
            valid_responses = [resp for resp in agent_responses 
                             if not any(error in str(resp) for error in error_indicators)]
            
            if not valid_responses:
                return 0.0
            
            # æ–¹æ³•1: æ£€æŸ¥å“åº”ä¸­æ•°å­—ç­”æ¡ˆçš„ä¸€è‡´æ€§
            response_numbers = []
            commitment_numbers = []
            
            # ä»æ¯ä¸ªå“åº”ä¸­æå–æ•°å­—
            for response in valid_responses:
                numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", response)
                if numbers:
                    response_numbers.append(numbers[-1])  # ä½¿ç”¨æœ€åä¸€ä¸ªæ•°å­—ä½œä¸ºç­”æ¡ˆ
            
            # ä»commitmentä¸­æå–æ•°å­—
            commit_numbers = re.findall(r"[+-]?\d+(?:\.\d+)?", commitment_text)
            if commit_numbers:
                commitment_numbers = commit_numbers[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ªæ•°å­—ä½œä¸ºç­”æ¡ˆ
            
            # è®¡ç®—æ•°å­—ä¸€è‡´æ€§å¥–åŠ±
            numerical_consistency = 0.0
            if response_numbers and commitment_numbers:
                # æ£€æŸ¥æ‰€æœ‰å“åº”çš„æ•°å­—æ˜¯å¦ä¸commitmentä¸€è‡´
                consistent_count = sum(1 for num in response_numbers if num == commitment_numbers)
                numerical_consistency = consistent_count / len(response_numbers)
            
            # æ–¹æ³•2: åŸºäºå“åº”ä¸commitmentçš„æ–‡æœ¬ç›¸ä¼¼åº¦
            text_similarity = 0.0
            if valid_responses and commitment_text:
                similarities = []
                for response in valid_responses:
                    # æ£€æŸ¥å“åº”é•¿åº¦ï¼Œå¤ªçŸ­çš„å“åº”ï¼ˆå¦‚é”™è¯¯æ¶ˆæ¯ï¼‰ç»™äºˆä½åˆ†
                    if len(response.strip()) < 10:
                        similarities.append(0.0)
                    else:
                        sim = self._calculate_text_similarity(response, commitment_text)
                        similarities.append(sim)
                
                if similarities:
                    text_similarity = np.mean(similarities)
            
            # æ–¹æ³•3: å“åº”é•¿åº¦å’Œè´¨é‡è¯„ä¼°
            quality_score = 0.0
            if valid_responses:
                quality_scores = []
                for response in valid_responses:
                    response_length = len(response.strip())
                    # åˆç†é•¿åº¦çš„å“åº”å¾—åˆ†æ›´é«˜
                    if 20 <= response_length <= 500:  # åˆç†é•¿åº¦èŒƒå›´
                        length_score = 1.0
                    elif 10 <= response_length <= 1000:  # å¯æ¥å—èŒƒå›´
                        length_score = 0.7
                    else:  # å¤ªçŸ­æˆ–å¤ªé•¿
                        length_score = 0.3
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨ç†è¿‡ç¨‹
                    has_reasoning = any(keyword in response.lower() 
                                      for keyword in ['first', 'then', 'therefore', 'because', 'so'])
                    reasoning_score = 0.3 if has_reasoning else 0.0
                    
                    quality_scores.append(length_score + reasoning_score)
                
                quality_score = np.mean(quality_scores)
            
            # ç»„åˆæ‰€æœ‰å› ç´ ï¼Œæ•°å­—ä¸€è‡´æ€§æƒé‡æœ€é«˜
            if numerical_consistency > 0:
                # å¦‚æœæœ‰æ•°å­—ä¸€è‡´æ€§ï¼Œä¸»è¦åŸºäºæ­¤è®¡ç®—
                al_reward = 0.6 * numerical_consistency + 0.25 * text_similarity + 0.15 * quality_score
            else:
                # å¦‚æœæ²¡æœ‰æ•°å­—ä¸€è‡´æ€§ï¼Œä¸»è¦åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦
                al_reward = 0.7 * text_similarity + 0.3 * quality_score
            
            return min(1.0, max(0.0, al_reward))
            
        except Exception as e:
            logger.warning(f"Error calculating AL reward: {e}")
            return 0.0

    def _calculate_collaborative_contribution_reward(self, final_answer: str, 
                                                   extra_info: Dict[str, Any], 
                                                   is_correct: bool) -> float:
        """
        è®¡ç®—åä½œè´¡çŒ®å¥–åŠ± r^CC
        åŸºäºä»¥ä¸‹å¯å‘å¼è§„åˆ™ï¼š
        1. å¦‚æœæœ€ç»ˆç­”æ¡ˆæ­£ç¡®ä¸”æ™ºèƒ½ä½“å“åº”å¤šæ ·åŒ–ï¼Œç»™äºˆé«˜å¥–åŠ±
        2. å¦‚æœæ™ºèƒ½ä½“å“åº”ä¸commitmentä¸€è‡´ï¼Œç»™äºˆä¸­ç­‰å¥–åŠ±
        3. è€ƒè™‘å“åº”çš„ç‹¬ç‰¹æ€§å’Œäº’è¡¥æ€§
        4. å¦‚æœAPIå¤±è´¥æˆ–å“åº”æ— æ•ˆï¼Œè¿”å›0.0
        """
        try:
            agent_responses = extra_info.get('agent_responses', [])
            commitment_text = extra_info.get('commitment_text', '')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰APIé”™è¯¯
            error_indicators = ['Error: Could not generate response', 'API Error', 'HTTP error', 'Failed to generate']
            
            # å¦‚æœæœ€ç»ˆç­”æ¡ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œè¿”å›0.0
            if any(error in str(final_answer) for error in error_indicators):
                return 0.0
            
            # å¦‚æœcommitmentåŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œè¿”å›0.0
            if any(error in str(commitment_text) for error in error_indicators):
                return 0.0
            
            # è¿‡æ»¤æ‰åŒ…å«é”™è¯¯çš„å“åº”
            valid_responses = []
            for response in agent_responses:
                if not any(error in str(response) for error in error_indicators):
                    valid_responses.append(response)
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆå“åº”ï¼Œè¿”å›0.0
            if not valid_responses:
                return 0.0
            
            # åŸºç¡€å¥–åŠ±ï¼šæ­£ç¡®æ€§è´¡çŒ®ï¼ˆåªæœ‰æ­£ç¡®æ—¶æ‰æœ‰åŸºç¡€å¥–åŠ±ï¼‰
            base_reward = 0.3 if is_correct else 0.0
            
            # å¤šæ ·æ€§å¥–åŠ±ï¼šæ™ºèƒ½ä½“å“åº”çš„å¤šæ ·æ€§ï¼ˆåªè€ƒè™‘æœ‰æ•ˆå“åº”ï¼‰
            diversity_reward = 0.0
            if len(valid_responses) > 1:
                unique_responses = len(set([resp.strip().lower() for resp in valid_responses]))
                diversity_ratio = unique_responses / len(valid_responses)
                diversity_reward = 0.3 * diversity_ratio
            
            # ä¸€è‡´æ€§å¥–åŠ±ï¼šä¸commitmentçš„ä¸€è‡´æ€§ï¼ˆåªè€ƒè™‘æœ‰æ•ˆå“åº”ï¼‰
            consistency_reward = 0.0
            if valid_responses and commitment_text:
                consistencies = []
                for response in valid_responses:
                    # æ£€æŸ¥å“åº”é•¿åº¦
                    if len(response.strip()) < 10:
                        consistencies.append(0.0)
                    else:
                        sim = self._calculate_text_similarity(response, commitment_text)
                        consistencies.append(sim)
                
                if consistencies:
                    avg_consistency = np.mean(consistencies)
                    consistency_reward = 0.2 * avg_consistency
            
            # è´¨é‡å¥–åŠ±ï¼šå¦‚æœæœ€ç»ˆç­”æ¡ˆåŒ…å«æ¨ç†è¿‡ç¨‹ä¸”ä¸åŒ…å«é”™è¯¯
            quality_reward = 0.0
            if final_answer and len(final_answer.strip()) > 10:
                # ç®€å•å¯å‘å¼ï¼šé•¿åº¦åˆç†ä¸”åŒ…å«æ•°å­¦æœ¯è¯­çš„ç­”æ¡ˆè´¨é‡æ›´é«˜
                answer_length = len(final_answer.split())
                has_reasoning = any(keyword in final_answer.lower() 
                                  for keyword in ['because', 'since', 'therefore', 'thus', 'so'])
                
                if 10 <= answer_length <= 200 and has_reasoning:
                    quality_reward = 0.2
                elif 5 <= answer_length <= 50:
                    quality_reward = 0.1
            
            total_cc_reward = base_reward + diversity_reward + consistency_reward + quality_reward
            return min(1.0, max(0.0, total_cc_reward))
            
        except Exception as e:
            logger.warning(f"Error calculating CC reward: {e}")
            return 0.0

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦
        ä½¿ç”¨TF-IDFå‘é‡å’Œä½™å¼¦ç›¸ä¼¼åº¦
        """
        try:
            if not text1.strip() or not text2.strip():
                return 0.0
            
            # ä½¿ç”¨TF-IDFå‘é‡åŒ–
            vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)
            texts = [text1, text2]
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            return float(max(0.0, min(1.0, similarity)))
            
        except Exception as e:
            logger.warning(f"Error calculating text similarity: {e}")
            return 0.0

    def get_env_info(self) -> Dict[str, Any]:
        # This info is used by the runner to setup the batch scheme.
        return {
            "episode_limit": self.episode_limit,
            "n_actions": int(self.n_actions),  # For discrete-action training (shapes only)
            "obs_shape": (self.max_question_length,), # For scheme vshape
            "state_shape": (1,), # Placeholder for scheme vshape
            # Any other info needed by the runner or learner
        }

    def render(self, mode='human'):
        if mode == 'human':
            if self.current_sample:
                print("-" * 30)
                print(f"Current Question: {self.current_question}")
                print(f"Ground Truth Answer: {self.current_ground_truth_answer}")
                print("-" * 30)
            else:
                print("No current sample to render. Call reset() first.")

    def close(self):
        # Clean up resources if any (e.g., closing file handles if dataset was local)
        logger.info("Closing HuggingFaceDatasetEnv.")
        pass

# Example Usage (for testing purposes):
if __name__ == '__main__':
    env_args_gsm8k = {
        "hf_dataset_path": "gsm8k",
        "hf_dataset_config_name": "main",
        "dataset_split": "test",
        "question_field_name": "question",
        "answer_field_name": "answer",
        "max_question_length": 1024,
        "max_answer_length": 200,
        "dataset_streaming": False, # For testing, non-streaming is easier
        "use_random_sampling": False, # Disable random sampling for deterministic testing
        "use_dataset_episode": False # Disable dataset-level episode for deterministic testing
    }
    
    # env = HuggingFaceDatasetEnv(**env_args_gsm8k)
    # obs, info = env.reset()
    # print("Observation (Question):", obs)
    # print("Ground Truth (from info['sample']):", info['sample'][env.answer_field])
    
    # # Simulate a step
    # dummy_action = "The answer is \\boxed{10}." 
    # next_obs, reward, terminated, truncated, step_info = env.step(dummy_action)
    # print(f"LLM's Action: {dummy_action}")
    # print(f"Next Obs: {next_obs}")
    # print(f"Reward: {reward}")
    # print(f"Terminated: {terminated}")
    # print(f"Step Info: {step_info}")
    # env.render()

    # env.reset() # Try another one
    # env.render()

    # Test MATH dataset
    env_args_math = {
        "hf_dataset_path": "competition_math",
        "dataset_split": "test", # Using test split which is smaller
        "question_field_name": "problem",
        "answer_field_name": "solution",
        "max_question_length": 2048,
        "max_answer_length": 2048,
        "dataset_streaming": False,
        "use_random_sampling": False, # Disable random sampling for deterministic testing
        "use_dataset_episode": False # Disable dataset-level episode for deterministic testing
    }
    math_env = HuggingFaceDatasetEnv(**env_args_math)
    obs, info = math_env.reset()
    math_env.render()
    # Simulate a step for MATH
    # For MATH, answers are more complex, often with LaTeX. Evaluation is harder.
    # Example: ground truth might be "\\boxed{-\\frac{1}{2}}"
    # dummy_math_action = "The final answer is \\boxed{-\\frac{1}{2}}"
    dummy_math_action = info['sample'][math_env.answer_field] # Give correct answer
    next_obs, reward, terminated, truncated, step_info = math_env.step(dummy_math_action)
    print(f"LLM's Action: {dummy_math_action}")
    print(f"Reward: {reward}") # Should be 1.0
    print(f"Step Info: {step_info}")
    math_env.render()

    # Test an incorrect answer for MATH
    obs, info = math_env.reset()
    math_env.render()
    dummy_math_action_wrong = "The final answer is \\boxed{42}"
    next_obs, reward, terminated, truncated, step_info = math_env.step(dummy_math_action_wrong)
    print(f"LLM's Action (Wrong): {dummy_math_action_wrong}")
    print(f"Reward: {reward}") # Should be 0.0
    print(f"Step Info: {step_info}")
    math_env.render() 