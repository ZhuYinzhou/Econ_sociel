runner: "episode_runner"
mac: "basic_mac"
learner: "q_learner"

# Stage1 (core) - STABLE preset to reduce fast collapse to majority class (2)
# Key ideas:
# - larger batch/buffer -> less per-step label noise
# - downweight class-2 explicitly -> make "always predict 2" expensive
# - turn off unnecessary regularizers for pure supervised stage

# Shorter smoke run first; increase after curves look sane
t_max: 50000
test_interval: 2000
test_nepisode: 2000
eval_dataset_split: "test"

n_agents: 3
belief_dim: 128
batch_size_run: 1

# Keep top-level action dim compatible with other stages
n_actions: 5
env_action_source: "discrete_action_boxed"

# === Stage1/2 supervised classification ===
train_belief_supervised: true
freeze_belief_encoder_in_supervised: true
use_mixer: false
# Disable soft-label supervision (gt_action_dist from `target_distribution_prob`) to avoid fast collapse to majority class.
belief_supervised_use_soft_labels: false
# Micro-batch the forward pass to avoid OOM while using a larger logical batch_size.
belief_supervised_micro_batch_size: 4

# ===== LRs / regularizers =====
# NOTE: supervised optimizer uses belief_net_lr (see q_learner.py)
lr: 0.0003
belief_net_lr: 0.0003
encoder_lr: 0.0
mixer_lr: 0.0
weight_decay: 0.0
lambda_sd: 0.0
lambda_m: 0.0

arch:
  entity_dim: 256
  attention_heads: 4
  transformer_blocks: 2
  key_dim: 64
  mlp_hidden_size: 256
  feedforward_size: 1024
  dropout_rate: 0.1
  layer_norm_epsilon: 0.00001

prompt_attention_heads: 2
commitment_embedding_dim: 1024

enable_llm_rollout: false
together_api_key: ""
coordinator_model: "gpt-4.1"
executor_model: "gpt-4.1"
llm_model_name: "gpt2"

env: "huggingface_dataset_env"
env_args:
  # merge e1+e2 (optionally add blm)
  hf_dataset_path:
    - "/home/zhuyinzhou/MAS/ECON/data/stage_1_dataset_metoo_e1"
    - "/home/zhuyinzhou/MAS/ECON/data/stage_1_dataset_metoo_e2"
    # - "/home/zhuyinzhou/MAS/ECON/data/stage_1_dataset_blm"
  dataset_split: "train"
  question_field_name: "question"
  answer_field_name: "answer"
  # Stage1/2 OOM-safe setting:
  # Attention memory scales as O(B * n_agents * heads * L^2). Reducing L is the most effective fix.
  # Start from 256, then try 384/512 if you have headroom.
  max_question_length: 256
  max_answer_length: 64
  dataset_streaming: false
  # Explicit: sample randomly each episode (avoid any ambiguity with defaults)
  use_random_sampling: true
  # Reduce terminal spam
  verbose_step_logging: false
  # Core users only
  filter_is_core_user: "core"
  # stance classification K=3
  n_actions: 3
  # train-only oversampling for class-1 (Oppose)
  oversample_enabled: true
  oversample_only_train: true
  oversample_label_id: 1
  oversample_target_ratio: 0.10
  oversample_seed: 42
  oversample_max_multiplier: 50

reward:
  al_weight: 0.0
  ts_weight: 1.0
  cc_weight: 0.0

train:
  # Bigger buffer/batch -> smoother gradients, less "pred0 flips 0/1"
  buffer_size: 256
  # Logical batch for replay sampling (effective_count â‰ˆ batch_size * n_agents). Forward is micro-batched above.
  batch_size: 64
  update_interval: 1
  # Ensure each supervised mini-batch contains at least K label-1 samples (if available in replay).
  supervised_min_label1_per_batch: 1
  optimizer: "adam"
  learning_rate: 0.001
  coordinator_learning_rate: 0.0005
  gamma: 0.99

system:
  use_cuda: true
  device_num: 0
  seed: 42
  debug: false

logging:
  use_tensorboard: true
  log_interval: 50
  save_model: true
  save_model_interval: 5000
  checkpoint_path: "./models"
  log_path: "./logs"
  experiment_name: "hisim-belief-core-stable"

# Supervised CE class weights (K=3: [0,1,2])
# Keep class-2 weight reasonable; otherwise the model can "never predict 2" (pred2_frac->0).
belief_supervised_class_weights: [1.8, 8.0, 1.5]

